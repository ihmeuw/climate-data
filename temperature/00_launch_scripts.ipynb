{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment used: earth_analytics_python (found here: /ihme/homes/nhashmeh/miniconda3/envs/earth-analytics-python/bin/python)\n",
    "\n",
    "# Load libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from shapely.geometry import Point\n",
    "from joblib import load\n",
    "import geopandas as gpd\n",
    "import os\n",
    "import xarray as xr\n",
    "from datetime import datetime, timedelta\n",
    "from joblib import load\n",
    "import getpass\n",
    "import subprocess\n",
    "import dask\n",
    "\n",
    "import sklearn\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import xgboost as xgb\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import median_absolute_error\n",
    "from sklearn.metrics import explained_variance_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.model_selection import cross_val_score \n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "os.getenv('SLURM_ARRAY_TASK_ID')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Merge datasets\n",
    "#### Here we launch a script that will combine ERA5 data, climate station data, and LCZ data. Things to know:\n",
    "- ERA5 data: 9km per pixel, downloaded at the 3-hour frequency level from the copernicus database. We then averaged these to daily values. The daily values can be found here: /mnt/share/erf/ERA5/three_hourly_temp_9km/daily_temps/\n",
    "#\n",
    "- Climate stations: Daily summary values from climate stations across the globe. We make use of the temperature averages as well as the cliamte station elevations in our model. Source: https://www.ncei.noaa.gov/metadata/geoportal/rest/metadata/item/gov.noaa.ncdc:C00652/html\n",
    "#\n",
    "- LCZ: Local Climate Zones that basically represent a categorical spectrum of urban development (and lack thereof). This dataset is originally at 100m. Source: https://lcz-generator.rub.de/global-lcz-map\n",
    "#\n",
    "- Output: year-month files containing table of points with climate station daily average temperatures, climate station elevation, daily averages of ERA5 temperature, and LCZ value. There are lat/lon and time variables here as well. Since climate stations are points, these basically dictate the spatial coverage of our new output. This table will be used to train the ML model.\n",
    "#\n",
    "- Resources/time: This takes a long time (how long?) and at least 500 GB of resources (750 GB?).\n",
    "#\n",
    "- Worker script: merge_era5_stations_script_v3.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "era5_path = '/mnt/share/erf/ERA5/three_hourly_temp_9km/daily_temps/'\n",
    "\n",
    "for file in os.listdir(era5_path):\n",
    "\n",
    "    year = file.split('_')[0] # get year\n",
    "\n",
    "    asdf = ['sbatch', '-J', '{}'.format(year), '-e', '/share/temp/sgeoutput/{}/errors/%x.e%j'.format(getpass.getuser()),\n",
    "        '-o', '/share/temp/sgeoutput/{}/output/%x.o%j'.format(getpass.getuser()), '-A', 'proj_diet',\n",
    "        '--mem=750G', '-c', '5', '-t', '240', '-C', 'archive', '-p', 'all.q', #'--wait',\n",
    "        '--wrap', '/ihme/homes/nhashmeh/miniconda3/envs/earth-analytics-python/bin/python ' + '/ihme/homes/nhashmeh/downscale_temperature/merge_era5_stations_script_v3.py ' + file]\n",
    "    # subprocess.call(asdf)\n",
    "    # # break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Creating the model\n",
    "#### Before we can create the model, the outputs from step 1 need to be combined into a single dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This file already exists, no need to run this script.\n"
     ]
    }
   ],
   "source": [
    "combine_outputs = False # set to True if you want this to run\n",
    "\n",
    "if combine_outputs == True:\n",
    "    filepath = '/mnt/share/erf/ERA5/merged_era5_with_stations/'\n",
    "    outpath = '/mnt/share/erf/ERA5/merged_era5_with_stations/ML_training/'\n",
    "\n",
    "    # ML_df = pd.DataFrame()\n",
    "    ML_df = []\n",
    "\n",
    "    counter = 0\n",
    "\n",
    "    now = datetime.now()\n",
    "    current_time = now.strftime(\"%I:%M:%S %p\")\n",
    "    print(\"Start\", current_time)\n",
    "\n",
    "    for count, file in enumerate(os.listdir(filepath), 1):\n",
    "        \n",
    "        if file.split('.')[-1] != 'feather':\n",
    "            continue\n",
    "\n",
    "        temp_df = pd.read_feather(filepath + file)\n",
    "\n",
    "        ML_df.append(temp_df)\n",
    "\n",
    "        if count % 50 == 0:\n",
    "            print(str(count) + \" files have been concatenated\")\n",
    "            \n",
    "    ML_df_final = pd.concat(ML_df).reset_index(drop=True)\n",
    "\n",
    "    # ML_df_final.to_feather(outpath + 'ML_df_all_v3.feather')\n",
    "\n",
    "    now = datetime.now()\n",
    "    current_time = now.strftime(\"%I:%M:%S %p\")\n",
    "    print(\"End\", current_time)\n",
    "\n",
    "else:\n",
    "    print(\"This file already exists, no need to run this script.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now that we have one table containing all of our temperature data, we can set it up for model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in the data if it isn't already loaded\n",
    "if 'ML_df_final' not in globals():\n",
    "    ML_df_final = pd.read_feather('/mnt/share/erf/ERA5/merged_era5_with_stations/ML_training/ML_df_all_v3.feather')\n",
    "\n",
    "# some final processing steps\n",
    "test_new = ML_df_final\n",
    "\n",
    "# capture cyclical behavior of annual data using sine/cosine\n",
    "test_new['day_of_year_sin'] = np.sin(2 * np.pi * test_new['day_of_year']/test_new['total_days_in_year'])\n",
    "test_new['day_of_year_cos'] = np.cos(2 * np.pi * test_new['day_of_year']/test_new['total_days_in_year'])\n",
    "\n",
    "# converting years to be relative to 1990 (so instead of 1990-2023, its 0-33) \n",
    "test_new['year'] = test_new['year'] - 1990\n",
    "\n",
    "# removing LCZ values of 0.0 (these represent the ocean)\n",
    "test_new = test_new[test_new['band_1'] != 0.0]\n",
    "\n",
    "# removing rows with -999.0 and -999.9 values, which come from climate station errors\n",
    "values_to_remove = [-999.0, -999.9]  # replace with your actual values\n",
    "test_new = test_new[~test_new['elevation'].isin(values_to_remove)]\n",
    "\n",
    "# renaming the band_1 column to lcz_value\n",
    "test_new.rename(columns={'band_1':'lcz_value'}, inplace=True)\n",
    "\n",
    "# creating a dummy variable for the lcz_value (this is our only dummy variable)\n",
    "test_new_one_dummy = pd.get_dummies(test_new, columns=['lcz_value'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Machine learning testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Score: 0.9816103832645074\n",
      "Mean Absolute Error: 1.1688360668087523\n",
      "Mean Squared Error: 2.912941802343664\n",
      "Root Mean Squared Error: 1.706734250650541\n",
      "Median Absolute Error: 0.8451612903225403\n",
      "Explained Variance Score: 0.9816103842954629\n"
     ]
    }
   ],
   "source": [
    "run_ML_testing = False\n",
    "\n",
    "if run_ML_testing == True:\n",
    "\n",
    "    # split the data into features (X) and target (y)\n",
    "    X_1 = test_new_one_dummy.drop(['temp', 'day_of_year'], axis=1)  # all columns except 'temp' (this is the predictor) and 'day_of_year' (already captured in sine/cosine)\n",
    "    y_1 = test_new_one_dummy['temp']  # 'temp' column\n",
    "\n",
    "    # split the data into train and test sets\n",
    "    X_train_1, X_test_1, y_train_1, y_test_1 = train_test_split(X_1, y_1, test_size=0.2, random_state=42)\n",
    "\n",
    "    # scale the features\n",
    "    scaler_1 = StandardScaler()\n",
    "    X_train_1 = scaler_1.fit_transform(X_train_1)\n",
    "    X_test_1 = scaler_1.transform(X_test_1)\n",
    "\n",
    "    ### Initialize the model ###\n",
    "    model_1 = DecisionTreeRegressor(max_depth=20)\n",
    "\n",
    "    ### Train the model ###\n",
    "    model_1.fit(X_train_1, y_train_1) # time varies based on model type and parameters (DTR with max_depth = 20 is ~ 20 minutes)\n",
    "\n",
    "    # Make predictions\n",
    "    predictions_1 = model_1.predict(X_test_1)\n",
    "\n",
    "    # Metrics\n",
    "    model_score_1 = model_1.score(X_test_1, y_test_1)\n",
    "    print(f\"Model Score: {model_score_1}\")\n",
    "\n",
    "    # calculate the mean absolute error of the model\n",
    "    mae_1 = mean_absolute_error(y_test_1, predictions_1)\n",
    "    print(f\"Mean Absolute Error: {mae_1}\")\n",
    "\n",
    "    # Mean Squared Error (MSE)\n",
    "    mse_1 = mean_squared_error(y_test_1, predictions_1)\n",
    "    print(f\"Mean Squared Error: {mse_1}\")\n",
    "\n",
    "    # Root Mean Squared Error (RMSE)\n",
    "    rmse_1 = np.sqrt(mse_1)\n",
    "    print(f\"Root Mean Squared Error: {rmse_1}\")\n",
    "\n",
    "    # Median Absolute Error (MedAE)\n",
    "    medae_1 = median_absolute_error(y_test_1, predictions_1)\n",
    "    print(f\"Median Absolute Error: {medae_1}\")\n",
    "\n",
    "    # Explained Variance Score\n",
    "    evs_1 = explained_variance_score(y_test_1, predictions_1)\n",
    "    print(f\"Explained Variance Score: {evs_1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Once we're happy with our model's in-sample performance, we can retrain the model without splitting up the data into train/test sets so that our final model is trained on all of our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_ML_full = False\n",
    "\n",
    "if run_ML_full == True:\n",
    "    \n",
    "    # scale the features\n",
    "    scaler = StandardScaler()\n",
    "    X_1_scaled = scaler.fit_transform(X_1)\n",
    "\n",
    "    # Initialize the model\n",
    "    model = DecisionTreeRegressor(max_depth=20)\n",
    "\n",
    "    # Train the model\n",
    "    model.fit(X_1_scaled, y_1)\n",
    "\n",
    "    # # Save the model to a file\n",
    "    # dump(model, 'validation_model_one_dummy_v3.joblib') \n",
    "\n",
    "    # # Save the scaler to a file\n",
    "    # dump(scaler, 'validation_scaler_one_dummy_v3.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Assemble the 1-km grid\n",
    "#### We need to create a global grid at the desired resolution (1 km), fill it with our variables, and use our model ro predict temperatures at that scale. First create the empty grid:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extent of our new grid (0.0089 degrees is approximately 1 km at the equator)\n",
    "new_lat = np.arange(-90, 90, 0.00898315)\n",
    "new_lon = np.arange(0, 359.9, 0.00898315)\n",
    "\n",
    "# create a new xarray dataset with the new lat/lon\n",
    "ds = xr.Dataset(\n",
    "    coords={\n",
    "        'latitude': (['latitude'], new_lat),\n",
    "        'longitude': (['longitude'], new_lon),\n",
    "    }\n",
    ")\n",
    "\n",
    "# convert the dataset to a geodataframe\n",
    "ds_df = ds.to_dataframe().reset_index()\n",
    "ds_df = gpd.GeoDataFrame(ds_df, geometry=gpd.points_from_xy(ds_df.longitude, ds_df.latitude))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now we fill the empty grid with our variables. First, the LCZ data:\n",
    "#### TODO: (THIS SHOULD BE A NON-INTERACTIVE PROCESS SINCE IT TAKES SEVERAL HOURS TO RUN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lcz_filepath = \"/mnt/share/erf/ERA5/merged_era5_with_stations/LCZ_data/lcz_resampled_mode.tif\"\n",
    "\n",
    "# Open the raster file\n",
    "raster = xr.open_rasterio(lcz_filepath)\n",
    "\n",
    "# Convert the DataArray to Dataset with a name\n",
    "raster_lcz = raster.to_dataset(name='lcz_value')\n",
    "\n",
    "raster_lcz = raster_lcz.rename({'x': 'longitude', 'y': 'latitude'})\n",
    "\n",
    "raster_lcz = raster_lcz.assign_coords(longitude=(((raster_lcz.longitude + 180) % 360) - 180))\n",
    "raster_lcz = raster_lcz.sortby(raster_lcz.longitude)\n",
    "raster_lcz = raster_lcz.assign_coords(longitude=(raster_lcz.longitude.where(raster_lcz.longitude >= 0, raster_lcz.longitude + 360)))\n",
    "\n",
    "lcz_df = raster_lcz.to_dataframe().reset_index()\n",
    "lcz_df.drop(columns='band', inplace=True)\n",
    "\n",
    "lcz_df = gpd.GeoDataFrame(lcz_df, geometry=gpd.points_from_xy(lcz_df.longitude, lcz_df.latitude))\n",
    "\n",
    "# now we join the empty grid with the LCZ data\n",
    "# WARNING: THIS TAKES SEVERAL HOURS -- IT MIGHT BE BETTER TO RUN THIS NON-INTERACTIVELY\n",
    "result = gpd.sjoin_nearest(ds_df, lcz_df)\n",
    "\n",
    "# remove rows where the lcz_value is 0 (these are the ocean)\n",
    "result = result[result['lcz_value'] != 0]\n",
    "\n",
    "# result.to_feather('/mnt/share/erf/ERA5/merged_era5_with_stations/grid_full_no_ocean.feather', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now we want to add elevation. Since elevation was originally a variable associated with the climate stations, we need a new source for elevation data in our empty grid. We opted to use the SRTM global dataset. Since this is very high resolution and distributed across many files, we are going to break up our grid into smaller chunks (5x5 degrees) to run merges in parallel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_filepath = '/mnt/share/erf/ERA5/merged_era5_with_stations/grid_setup/grid_full_no_ocean.feather'\n",
    "\n",
    "grid_csv = pd.read_csv(grid_filepath)\n",
    "\n",
    "now = datetime.now()\n",
    "current_time = now.strftime(\"%I:%M:%S %p\")\n",
    "print(\"Start\", current_time)\n",
    "\n",
    "# Define your latitude and longitude boundaries for your chunks.\n",
    "lat_bounds = list(range(-60, 81, 5))\n",
    "lon_bounds = list(range(0, 361, 5))\n",
    "\n",
    "# Load the entire dataset (if it's too large, consider loading in chunks and processing each chunk separately)\n",
    "# data = pd.read_csv('file.csv')\n",
    "data = grid_csv\n",
    "\n",
    "# Split the dataset into chunks by latitude and longitude\n",
    "for i in range(len(lat_bounds)-1):\n",
    "    print(\"Searching between latitudes \" + str(lat_bounds[i]) + \" and \" + str(lat_bounds[i+1]))\n",
    "    for j in range(len(lon_bounds)-1):\n",
    "        chunk = data[(data['latitude_left'] >= lat_bounds[i]) & (data['latitude_left'] < lat_bounds[i+1]) & \n",
    "                     (data['longitude_left'] >= lon_bounds[j]) & (data['longitude_left'] < lon_bounds[j+1])]\n",
    "        \n",
    "        if len(chunk) > 0:\n",
    "            now = datetime.now()\n",
    "            current_time = now.strftime(\"%I:%M:%S %p\")\n",
    "            print(\"bounds satisfied, saving chunk at time\", current_time)\n",
    "            \n",
    "            filename = \"chunk_lat\" + str(lat_bounds[i]) + \"_lat\" + str(lat_bounds[i+1]) + \"_lon\" + str(lon_bounds[j]) + \"_lon\" + str(lon_bounds[j+1]) + \".feather\"\n",
    "\n",
    "            # chunk.reset_index(drop=True).to_feather(\"/mnt/share/erf/ERA5/merged_era5_with_stations/grid_setup/chunks_by_latlon_v3/\" + filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now that our data is chunked, we can add elevations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks_path = \"/mnt/share/erf/ERA5/merged_era5_with_stations/grid_setup/chunks_by_latlon_v3/\"\n",
    "\n",
    "for file in os.listdir(chunks_path):\n",
    "\n",
    "    chunk_number = file.split('.')[0] # get year\n",
    "\n",
    "    asdf = ['sbatch', '-J', '{}'.format(chunk_number), '-e', '/share/temp/sgeoutput/{}/errors/%x.e%j'.format(getpass.getuser()),\n",
    "        '-o', '/share/temp/sgeoutput/{}/output/%x.o%j'.format(getpass.getuser()), '-A', 'proj_diet',\n",
    "        '--mem=300G', '-c', '5', '-t', '360', '-p', 'all.q', #'--wait',\n",
    "        '--wrap', '/ihme/homes/nhashmeh/miniconda3/envs/earth-analytics-python/bin/python ' + '/ihme/homes/nhashmeh/downscale_temperature/grid_setup/grid_setup_with_elevation_v3.py ' + file]\n",
    "    # subprocess.call(asdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TODO: THERE WERE FIXES NEEDED AFTER THIS STEP. SHOULD INCORPORATE THESE INTO THE ABOVE\n",
    "#### TODO: UPDATE THE WORKER SCRIPT TO OUTPUT THE FILES AS COMPRESSED NETCDF FILES?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks_elev_path = \"/mnt/share/erf/ERA5/merged_era5_with_stations/grid_setup/chunks_with_elevation_and_era5_v3/\"\n",
    "\n",
    "for file in os.listdir(chunks_elev_path):\n",
    "\n",
    "    chunk_number = file.split('.')[0] # get chunk\n",
    "\n",
    "    asdf = ['sbatch', '-J', '{}'.format(chunk_number), '-e', '/share/temp/sgeoutput/{}/errors/%x.e%j'.format(getpass.getuser()),\n",
    "        '-o', '/share/temp/sgeoutput/{}/output/%x.o%j'.format(getpass.getuser()), '-A', 'proj_erf',\n",
    "        '--mem=100G', '-c', '5', '-t', '360', '-p', 'all.q', #'--wait',\n",
    "        '--wrap', '/ihme/homes/nhashmeh/miniconda3/envs/earth-analytics-python/bin/python ' + '/ihme/homes/nhashmeh/downscale_temperature/grid_setup/fixes_v3.py ' + file]\n",
    "    # subprocess.call(asdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now we add the ERA5 data. The outputs of this script are now ready for predictions.\n",
    "\n",
    "#### TODO: UPDATE THE WORKER SCRIPT TO OUTPUT THE FILES AS COMPRESSED NETCDF FILES?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks_elev_path = \"/mnt/share/erf/ERA5/merged_era5_with_stations/grid_setup/chunks_with_elevation_v3/\"\n",
    "era5_path = \"/mnt/share/erf/ERA5/three_hourly_temp_9km/daily_temps/\"\n",
    "\n",
    "for file in os.listdir(chunks_elev_path):\n",
    "\n",
    "    chunk_number = file.split('.')[0] # get chunk\n",
    "\n",
    "    for era5_file in os.listdir(era5_path):\n",
    "\n",
    "        year = era5_file.split('_')[0]\n",
    "\n",
    "        jname = chunk_number + '_' + year\n",
    "\n",
    "        asdf = ['sbatch', '-J', '{}'.format(jname), '-e', '/share/temp/sgeoutput/{}/errors/%x.e%j'.format(getpass.getuser()),\n",
    "            '-o', '/share/temp/sgeoutput/{}/output/%x.o%j'.format(getpass.getuser()), '-A', 'proj_erf',\n",
    "            '--mem=250G', '-c', '5', '-t', '480', '-p', 'all.q', #'--wait',\n",
    "            '--wrap', '/ihme/homes/nhashmeh/miniconda3/envs/earth-analytics-python/bin/python ' + '/ihme/homes/nhashmeh/downscale_temperature/grid_setup/grid_setup_with_era5_v3.py ' + file + ' ' + era5_file]\n",
    "        # subprocess.call(asdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4: Run predictions\n",
    "\n",
    "#### The final two steps are: \n",
    "#### 1. Run predictions for each chunk\n",
    "#### 2. Recompile chunks with predictions into a daily output.\n",
    "\n",
    "#### This can be done in the launch_regional_predictions.ipynb, but maybe I should bring those cells here...\n",
    "\n",
    "#### Run predictions for each chunk:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################\n",
    "year = 1999 # CHANGE THIS FOR EACH YEAR YOU WANT TO RUN\n",
    "##########################\n",
    "\n",
    "path_to_chunks = '/mnt/share/erf/ERA5/merged_era5_with_stations/grid_setup/chunks_with_elevation_and_era5_v3/'\n",
    "path_to_chunks_year = path_to_chunks + str(year) + '/'\n",
    "output_path = '/mnt/share/erf/ERA5/merged_era5_with_stations/grid_setup/predictions/year_' + str(year) + '_predictions/'\n",
    "\n",
    "list_of_files = [name for name in os.listdir(path_to_chunks_year) if os.path.isfile(os.path.join(path_to_chunks_year, name))]\n",
    "\n",
    "# count the number of files in this directory\n",
    "n_files = len([name for name in os.listdir(path_to_chunks_year) if os.path.isfile(os.path.join(path_to_chunks_year, name))])\n",
    "\n",
    "# check the number of files and the list of files\n",
    "print(n_files)\n",
    "print(list_of_files)\n",
    "\n",
    "# make output_path if it doesn't exist\n",
    "if not os.path.exists(output_path):\n",
    "    os.makedirs(output_path)\n",
    "\n",
    "# Create a job array with one job per file\n",
    "sbatch_command = ['sbatch', '-J', 'regional_predictions', '--array=0-' + str(n_files-1) + '%150',\n",
    "                  '-e', '/share/temp/sgeoutput/{}/errors/%x.e%j'.format(getpass.getuser()),\n",
    "                  '-o', '/share/temp/sgeoutput/{}/output/%x.o%j'.format(getpass.getuser()), '-A', 'proj_erf',\n",
    "                  '--mem=50G', '-c', '5', '-t', '240', '-p', 'all.q',\n",
    "                  '--wrap', '/ihme/homes/nhashmeh/miniconda3/envs/earth-analytics-python/bin/python ' + \n",
    "                      '/mnt/share/code/nhashmeh/downscaling/temperature/regional_predictions.py ' + \n",
    "                      '${SLURM_ARRAY_TASK_ID}' + ' ' + str(year) + ' ' + path_to_chunks_year + ' ' + output_path]\n",
    "\n",
    "print(' '.join(sbatch_command))  # Print the sbatch command\n",
    "# subprocess.Popen(sbatch_command)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Recompile chunks with predictions into a daily output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## This launches a script for each day of the year. It does the following:\n",
    "## 1. Reads in each file from the initial prediction output (a 5x5 degree chunk of data for a whole year)\n",
    "## 2. Extracts the data for the day of interest, does some processing, and appended to a larger list\n",
    "## 3. All data is concatenated and then converted to an xarray Dataset\n",
    "## 4. Writes the xarray Dataset to a netCDF file using some compression parameters\n",
    "\n",
    "year = 2000\n",
    "\n",
    "# if year is a leap year, the number of days in the year is 366, otherwise it is 365\n",
    "if year % 4 == 0:\n",
    "    num_days = 366\n",
    "else:\n",
    "    num_days = 365\n",
    "\n",
    "# create a list of dates for the year of interest\n",
    "day_list = np.arange(1, num_days + 1) \n",
    "\n",
    "input_path = '/mnt/share/erf/ERA5/merged_era5_with_stations/grid_setup/predictions/year_' + str(year) + '_predictions/'\n",
    "output_path = '/mnt/share/erf/ERA5/merged_era5_with_stations/grid_setup/predictions/by_day_nc/' + str(year) + '/'\n",
    "\n",
    "# make output_path if it doesn't exist\n",
    "if not os.path.exists(output_path):\n",
    "    os.makedirs(output_path)\n",
    "\n",
    "for day in day_list:\n",
    "\n",
    "    sbatch_command = ['sbatch', '-J', str(year) + '_' + str(day),\n",
    "                      '-e', '/share/temp/sgeoutput/{}/errors/%x.e%j'.format(getpass.getuser()),\n",
    "                      '-o', '/share/temp/sgeoutput/{}/output/%x.o%j'.format(getpass.getuser()), '-A', 'proj_erf',\n",
    "                      '--mem=50G', '-c', '5', '-t', '240', '-p', 'all.q',\n",
    "                      '--wrap', '/ihme/homes/nhashmeh/miniconda3/envs/earth-analytics-python/bin/python ' + \n",
    "                        #   '/mnt/share/code/nhashmeh/downscaling/temperature/compile_by_day_nc.py ' + \n",
    "                      '/mnt/share/code/nhashmeh/downscaling/temperature/daily_preds_nc.py ' + \n",
    "                          '${SLURM_ARRAY_TASK_ID}' + ' ' + str(day) + ' ' + str(year) + ' ' + input_path + ' ' + output_path]\n",
    "\n",
    "    print(' '.join(sbatch_command))  # Print the sbatch command\n",
    "    # subprocess.Popen(sbatch_command)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## That is the end of the pipeline!\n",
    "#### TODO: Address problematic outliers in final data. I think this should be addressed right before the chunks with predictions are compiled into daily outputs. However, its easier to fix it from here rather than rerunning that part of the pipeline."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "earth-analytics-python",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
